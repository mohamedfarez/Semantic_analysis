# -*- coding: utf-8 -*-
"""Semantic-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KwAANY1vb72rmPDpNOmcKGtLihI7Z9Hk

# **Imports**
"""

import pandas as pd

from matplotlib import pyplot as plt
import seaborn as sns

from transformers import AutoTokenizer

!pip install datasets
!pip install umap-learn

"""Using data from **huggingface** dataset"""

# i will use emotion from it
from datasets import load_dataset
emotions = load_dataset("emotion")

emotions  #emotions have 3 sets like you ask (train:16000 ,validation:2000 ,test : 2000)

# i will pass train data to train_data
train_data=emotions['train']

# i will use feature  to know more information about data as U see ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']
train_data.features

# try to see the first 5 elements in data
train_data[:5]

"""# **Convert data to DataFrames**"""

emotions.set_format(type="pandas")
df = emotions["train"][:]

# see head of data
df.head()

# @title data
df['label'].plot(kind='hist', bins=20, title='label')
plt.gca().spines[['top', 'right',]].set_visible(False)

# make label name to undersand the data
def label_int2str(row):
    return emotions["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)

#see head of data to make sure everything is okay
df.head()

# @title emotions
df.groupby('label_name').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

#to make it usabel to use with tokens
emotions.reset_format()

"""# **Tokens**"""

# Define the model checkpoint to use a pre-trained DistilBERT model
model_ckpt = "distilbert-base-uncased"

# Load the tokenizer corresponding to the pre-trained DistilBERT model
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

# Define a function to tokenize a batch of text data
# Use the tokenizer to process the text data
# The 'padding=True' argument pads the sequences to the same length
# The 'truncation=True' argument truncates sequences to the maximum length allowed by the model
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

"""Apply the `tokenize` function to the `emotions` dataset using the `map` method.
The `batched=True` argument allows the `tokenize` function to process multiple examples at once, improving efficiency by batching the tokenization process.
Setting `batch_size=None` means that the entire dataset will be processed in a single batch. The result is an updated dataset with tokenized inputs, where each example is now tokenized and padded as needed.
"""

emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

print(emotions_encoded["train"].column_names)

"""# **Model Initialization**"""

from transformers import AutoModel
import torch

model_ckpt = "distilbert-base-uncased"
## Determine if a GPU is available; if so, set the device to CUDA (GPU), otherwise use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#Load the pre-trained model specified by `model_ckpt`
model = AutoModel.from_pretrained(model_ckpt).to(device)

def extract_hidden_states(batch):
    # Place model inputs on the GPU
    inputs = {k:v.to(device) for k,v in batch.items()
              if k in tokenizer.model_input_names}
    # Extract last hidden states
    with torch.no_grad():
        last_hidden_state = model(**inputs).last_hidden_state
    # Return vector for [CLS] token
    return {"hidden_state": last_hidden_state[:,0].cpu().numpy()}

emotions_encoded.set_format("torch",
                            columns=["input_ids", "attention_mask", "label"])

emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

from huggingface_hub import notebook_login

notebook_login()

from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(emotions_encoded["train"]) // batch_size
model_name = f"{model_ckpt}-finetuned-emotion"
training_args = TrainingArguments(
    output_dir=model_name,
    num_train_epochs=2,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    disable_tqdm=False,
    logging_steps=logging_steps,
    push_to_hub=True,
    log_level="error"
)

from transformers import AutoModelForSequenceClassification

# Replace the original model loading with AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=len(label2id))

# Rest of your code remains the same
trainer = Trainer(model=model,
                  args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=emotions_encoded["train"],
                  eval_dataset=emotions_encoded["validation"],
                  tokenizer=tokenizer)

trainer.train();

preds_output = trainer.predict(emotions_encoded["validation"])
preds_output.metrics

y_preds = np.argmax(preds_output.predictions, axis=1)
plot_confusion_matrix(y_preds, y_valid, labels)

# Evaluate RoBERTa
preds_output_roberta = trainer_roberta.predict(emotions_encoded_roberta["validation"])
metrics_roberta = preds_output_roberta.metrics
print("RoBERTa Metrics:", metrics_roberta)

# Evaluate DistilBERT (assuming 'preds_output' is available)
metrics_distilbert = preds_output.metrics
print("DistilBERT Metrics:", metrics_distilbert)

# Compare metrics
print("Comparison of Metrics:")
#comparing the test accuracy.
print("Accuracy - DistilBERT:", metrics_distilbert["test_accuracy"], ", RoBERTa:", metrics_roberta["test_accuracy"])

print("F1 Score - DistilBERT:", metrics_distilbert.keys(), ", RoBERTa:", metrics_roberta.keys())

